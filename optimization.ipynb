{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-lecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "# coding=utf-8\n",
    "import scipy.optimize as optimize\n",
    "from numpy import genfromtxt\n",
    "\n",
    "def manual_gd(f_, x_old=0, x_new=5, learningRate=0.1, precision=0.00001):\n",
    "    \"\"\"\n",
    "    A simple gradient descent usage for function optimization\n",
    "    \"\"\"\n",
    "    iteration = 0\n",
    "    while abs(x_new - x_old) > precision:\n",
    "        iteration += 1\n",
    "        x_old = x_new\n",
    "        x_new = x_old - (learningRate * f_(x_old))\n",
    "    print(\"MGD\\ti=%d\\tf(x)=%f\" % (iteration, x_new))\n",
    "\n",
    "def f(x): return x**2 - 4 * x + 1\n",
    "\n",
    "def f_(x): return 2 * x - 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-integer",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hundred-tunnel",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = genfromtxt(\"data/spring.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acknowledged-madness",
   "metadata": {},
   "source": [
    "### Using self-implemented methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focused-oliver",
   "metadata": {},
   "source": [
    "#### linear regression with gradient descent\n",
    "\n",
    "A [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) and [linear regression](https://en.wikipedia.org/wiki/Linear_regression) example to solve _y = mx + b_ equation using [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) (_m_ is slope, _b_ is y-intercept).\n",
    "\n",
    "Source: [Matt Nedrich](http://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "falling-alcohol",
   "metadata": {},
   "source": [
    "_step_gradient_ uses partial derivatives of each parameter (_m_ and _b_) of the error function _l_rate_ variable controls how large of a step we take downhill during each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "photographic-going",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_gradient(b, m, points, l_rate):\n",
    "    b_ = 0\n",
    "    m_ = 0\n",
    "    N = len(points)\n",
    "    for i in range(0, N):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        b_ += -(2.0 / N) * (y - ((m * x) + b))\n",
    "        m_ += -(2.0 / N) * x * (y - ((m * x) + b))\n",
    "    b1 = b - (l_rate * b_)\n",
    "    m1 = m - (l_rate * m_)\n",
    "    return [b1, m1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-fishing",
   "metadata": {},
   "source": [
    "_computer_error_ computes the error using Mean Squared Error (MSE):\n",
    "$$MSE = \\frac{1}{N}\\sum(yi-(mxi+b))^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isolated-plenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error(b, m, points):\n",
    "    error = 0\n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        error += (y - (m * x + b)) ** 2\n",
    "    return error / float(len(points))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "through-summary",
   "metadata": {},
   "source": [
    "Now we can apply GD over data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thousand-electric",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gd(points):\n",
    "    # initial y-intercept guess\n",
    "    b0 = 0\n",
    "    # initial slope guess\n",
    "    m0 = 0\n",
    "    # number of iterations to perform the GD\n",
    "    n_iter = 1000\n",
    "    for i in range(n_iter):\n",
    "        # perform GD iterations\n",
    "        b0, m0 = step_gradient(b0, m0, points, 0.0001)\n",
    "    print(\n",
    "        \"GD\\ti=%d\\tb=%f\\tm=%f\\te=%f\\t(y=%f*x+%f)\"\n",
    "        % (n_iter, b0, m0, compute_error(b0, m0, points), m0, b0)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "younger-staff",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gd(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interstate-pledge",
   "metadata": {},
   "source": [
    "#### function optimization with gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connected-medium",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_gd(f_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binding-messenger",
   "metadata": {},
   "source": [
    "#### root finding methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arbitrary-failure",
   "metadata": {},
   "source": [
    "##### [Bisection method](https://en.wikipedia.org/wiki/Bisection_method) for root finding\n",
    "\n",
    "<https://en.wikipedia.org/wiki/Bisection_method>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incoming-segment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bisect(f, x0, x1, e=1e-12):\n",
    "    x = (x0 + x1) / 2.0\n",
    "    n = 0\n",
    "    while (x1 - x0) / 2.0 > e:\n",
    "        n += 1\n",
    "        if f(x) == 0:\n",
    "            return x\n",
    "        elif f(x0) * f(x) < 0:\n",
    "            x1 = x\n",
    "        else:\n",
    "            x0 = x\n",
    "        x = (x0 + x1) / 2.0\n",
    "    print(f\"B\\ti={n}\\tx={x}\\tf(x)={f(x)}\")\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-lottery",
   "metadata": {},
   "outputs": [],
   "source": [
    "bisect(f, 0, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "august-headline",
   "metadata": {},
   "source": [
    "##### [Secant method](https://en.wikipedia.org/wiki/Secant_method) for root finding\n",
    "\n",
    "<https://en.wikipedia.org/wiki/Secant_method>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-romantic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def secant(f, x0, x1, e=1e-12):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    n = 0\n",
    "    while True:\n",
    "        n += 1\n",
    "        x = x1 - f(x1) * ((x1 - x0) / (f(x1) - f(x0)))\n",
    "        if abs(x - x1) < e:\n",
    "            break\n",
    "        x0 = x1\n",
    "        x1 = x\n",
    "    print(\"S\\ti=%d\\tx=%f\\tf(x)=%f\" % (n, x, f(x)))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subject-offset",
   "metadata": {},
   "outputs": [],
   "source": [
    "secant(f, 1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "straight-parking",
   "metadata": {},
   "source": [
    "##### [Newton's method](https://en.wikipedia.org/wiki/Newton%27s_method) for root finding\n",
    "\n",
    "<https://en.wikipedia.org/wiki/Newton%27s_method>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wound-softball",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton(f, f_, x0, e=1e-12):\n",
    "    n = 0\n",
    "    while True:\n",
    "        n += 1\n",
    "        x = x0 - (f(x0) / f_(x0))\n",
    "        if abs(x - x0) < e:\n",
    "            break\n",
    "        x0 = x\n",
    "    print(f\"N\\ti={n}\\tx={x}\\tf(x)={f(x)}\")\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-qatar",
   "metadata": {},
   "outputs": [],
   "source": [
    "newton(f, f_, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capital-auditor",
   "metadata": {},
   "source": [
    "### Using scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "herbal-system",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Bisection\\t\\tx=%f\" % optimize.bisect(f, 0, 2))\n",
    "print(\"Secant\\t\\t\\tx=%f\" % optimize.newton(f, 1))\n",
    "print(\"Newton's\\t\\tx=%f\" % optimize.newton(f, 1, f_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
