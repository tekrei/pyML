{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "organized-religion",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "- <https://towardsdatascience.com/logistic-regression-from-scratch-69db4f587e17>\n",
    "- <https://aihubprojects.com/logistic-regression-from-scratch/>\n",
    "- <https://thelaziestprogrammer.com/sharrington/math-of-machine-learning/solving-logreg-newtons-method>\n",
    "- <https://www.pyimagesearch.com/2016/10/17/stochastic-gradient-descent-sgd-with-python/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifth-playback",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import matplotlib.pyplot as plot\n",
    "import numpy as np\n",
    "from utility import display, load_dataset, sigmoid, split_dataset\n",
    "from sklearn.linear_model import LogisticRegression as ScikitLogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explicit-prescription",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The following class contains model fitting and prediction methods using Logistic Regression\n",
    "using [Gradient Descent](https://en.wikipedia.org/wiki/Gradient_descent) with [sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) and [Log Loss](https://en.wikipedia.org/wiki/Log_loss) function.\n",
    "\n",
    "- Log Loss (`log_loss`): $$L_{\\log}(y, p) = -(y \\log (p) + (1 - y) \\log (1 - p))$$\n",
    "- Sum of Squared Errors (SSE) (`sse_loss`): $$SSE(y, p) = \\sum_{i}^{n} (p - y)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-earthquake",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Methods(Enum):\n",
    "    GD = 0\n",
    "    SGD = 1\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, verbose = False):\n",
    "        self.verbose = verbose\n",
    "        self.weights = np.zeros(0)\n",
    "\n",
    "    @staticmethod\n",
    "    def log_loss(p, y):\n",
    "        # log loss function\n",
    "        # Note: you can also use https://scikit-[Log Loss from scikit-learn](learn.org/stable/modules/generated/sklearn.metrics.log_loss.html)\n",
    "        # clip for overflow\n",
    "        p = np.clip(p, 1e-15, 1 - 1e-15)\n",
    "        # calculate log loss\n",
    "        return (-y * np.log(p) - (1 - y) * np.log(1 - p)).mean()\n",
    "    \n",
    "    @staticmethod\n",
    "    def sse_loss(p, y):\n",
    "        # sum of squared errors loss function\n",
    "        return  np.sum((p - y) ** 2)\n",
    "\n",
    "    def next_batch(self, x, y, batch_size):\n",
    "        # loop over our dataset `X` in mini-batches of size `batchSize`\n",
    "        for i in np.arange(0, x.shape[0], batch_size):\n",
    "            # yield a tuple of the current batched data and labels\n",
    "            yield (x[i:i + batch_size], y[i:i + batch_size])\n",
    "    \n",
    "    def fit(self, x, y, method=Methods.GD, lr=0.05, epochs=10000, batch_size = 100, tolerance=1e-06):\n",
    "        return self.gd(x, y, lr, epochs) if method is Methods.GD else self.sgd(x, y, lr, epochs, batch_size, tolerance) \n",
    "    \n",
    "    def sgd(self, x, y, lr, epochs, batch_size, tolerance):\n",
    "        # weights initialization\n",
    "        self.weights = np.zeros(x.shape[1])\n",
    "        loss_values = []\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            epoch_loss = []\n",
    "            for (bx, by) in self.next_batch(x, y, batch_size):\n",
    "                # calculate sigmoid\n",
    "                yp = self.calculate(bx)\n",
    "\n",
    "                # calculate the error\n",
    "                error = yp - by\n",
    "                \n",
    "                # calculate the gradient\n",
    "                gradient = np.dot(bx.T, error) / by.size\n",
    "\n",
    "                # update weights\n",
    "                self.weights -= lr * gradient\n",
    "\n",
    "                # calculate new sigmoid\n",
    "                yp = self.calculate(bx)\n",
    "\n",
    "                # calculate the loss\n",
    "                loss = self.sse_loss(yp, by)\n",
    "\n",
    "                # display loss\n",
    "                if self.verbose and i % 1000 == 0:\n",
    "                    print(f'loss in iteration {i} -> {loss} \\t')\n",
    "\n",
    "                # collect loss values\n",
    "                epoch_loss.append(loss)\n",
    "            loss_values.append(np.average(epoch_loss))\n",
    "        # return loss values\n",
    "        return loss_values\n",
    "\n",
    "    def gd(self, x, y, lr, epochs=10000):\n",
    "        # weights initialization\n",
    "        self.weights = np.zeros(x.shape[1])\n",
    "        loss_values = []\n",
    "\n",
    "        for i in range(epochs):\n",
    "            # calculate sigmoid using currrent weights\n",
    "            yp = self.calculate(x)\n",
    "            \n",
    "            # calculate the error\n",
    "            error = yp - y\n",
    "\n",
    "            # calculate the gradient\n",
    "            gradient = np.dot(x.T, error) / y.size\n",
    "\n",
    "            # update weights\n",
    "            self.weights -= lr * gradient\n",
    "\n",
    "            # calculate sigmoid using new weights\n",
    "            yp = self.calculate(x)\n",
    "\n",
    "            # calculate the loss\n",
    "            loss = self.log_loss(yp, y)\n",
    "\n",
    "            # display loss\n",
    "            if self.verbose and i % 1000 == 0:\n",
    "                print(f'loss in iteration {i} -> {loss} \\t')\n",
    "\n",
    "            # collect loss values\n",
    "            loss_values.append(loss)\n",
    "        # return loss values\n",
    "        return loss_values\n",
    "    \n",
    "    def calculate(self, x):\n",
    "        # Calculate x * W\n",
    "        z = np.dot(x, self.weights)\n",
    "        # Calculate the sigmoid\n",
    "        return sigmoid(z)\n",
    "\n",
    "    def predict(self, x):\n",
    "        # Predict using sigmoid calculation and return binary result\n",
    "        return self.calculate(x).round()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saved-packet",
   "metadata": {},
   "source": [
    "load training and test data from CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medieval-plymouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, target = load_dataset(\"data/forestfires.csv\")\n",
    "train_x, train_y, test_x, test_y = split_dataset(dataset, target, 0.75)\n",
    "print(f\"Training set size: {len(train_x)}, Testing set size: {len(test_x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-expansion",
   "metadata": {},
   "source": [
    "train the model with training data using Gradient Descent and plot loss during iteration and plot confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blind-respondent",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "loss_plot = plot.plot(model.fit(train_x, train_y, Methods.GD))\n",
    "predictions = model.predict(test_x)\n",
    "display(test_y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "buried-fundamental",
   "metadata": {},
   "source": [
    "train the model with training data using Stochastic Gradient Descent and plot loss during iteration and plot confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-liver",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "loss_plot = plot.plot(model.fit(train_x, train_y, Methods.SGD))\n",
    "predictions = model.predict(test_x)\n",
    "display(test_y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "little-principle",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Use scikit-learn [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bizarre-heritage",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = ScikitLogisticRegression()\n",
    "model.fit(train_x, train_y)\n",
    "predictions = model.predict(test_x)\n",
    "display(test_y, predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
